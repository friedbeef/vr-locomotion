{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OM4ko4PpgTP9",
        "outputId": "8beb5ca1-59cd-432e-bbc0-4516b49a2414"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/hanlin/opt/anaconda3/envs/opencv/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "[2024-05-10 23:02:53] INFO (torcheeg/MainThread) üîç | Detected cached processing results, reading cache from /Users/hanlin/Desktop/vr_locomotion/seed.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torcheeg import transforms\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torcheeg.datasets import SEEDDataset\n",
        "from torcheeg.models import DGCNN\n",
        "from dgcnn_gat_a import DGCNN\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "\n",
        "#TODO: datasetÊé•ÂèóÊî∂ÈõÜÁöÑÊï∞ÊçÆ\n",
        "dataset = SEEDDataset(io_path='/Users/hanlin/Desktop/vr_locomotion/seed',  # ËÆæÁΩÆ‰∏∫‰πãÂâç‰øùÂ≠òÊï∞ÊçÆÁöÑË∑ØÂæÑ\n",
        "                      offline_transform=transforms.BandDifferentialEntropy(band_dict={\n",
        "                          \"delta\": [1, 4],\n",
        "                          \"theta\": [4, 8],\n",
        "                          \"alpha\": [8, 14],\n",
        "                          \"beta\": [14, 31],\n",
        "                          \"gamma\": [31, 49]\n",
        "                      }),\n",
        "                      online_transform=transforms.Compose([\n",
        "                          transforms.ToTensor()\n",
        "                      ]),\n",
        "                      label_transform=transforms.Compose([\n",
        "                          transforms.Select('emotion'),\n",
        "                          transforms.Lambda(lambda x: x + 1)\n",
        "                      ]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([62, 5])\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "# The data format the model receives\n",
        "sample, label = dataset[0]\n",
        "print(sample.shape)\n",
        "print(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Change here to use our data\n",
        "dataset_using = dataset\n",
        "\n",
        "# split\n",
        "train_ratio = 0.7\n",
        "val_ratio = 0.15\n",
        "test_ratio = 0.15\n",
        "\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "train_size = int(train_ratio * len(dataset_using))\n",
        "val_size = int(val_ratio * len(dataset_using))\n",
        "test_size = len(dataset_using) - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset_using, [train_size, val_size, test_size])\n",
        "\n",
        "# TODO: hyper params\n",
        "batch_size = 32\n",
        "num_epochs = 65\n",
        "# l1_reg = 0\n",
        "# l2_reg = 0\n",
        "learning_rate = 0.0001\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "model = DGCNN(in_channels=5, num_electrodes=62, hid_channels=32, num_classes=3)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for batch_data, batch_labels in train_loader:\n",
        "        outputs = model(batch_data)\n",
        "        \n",
        "        ce_loss = criterion(outputs, batch_labels)\n",
        "        \n",
        "        # l1_reg_loss = 0\n",
        "        # for param in model.parameters():\n",
        "        #     l1_reg_loss += torch.sum(torch.abs(param))\n",
        "        # loss = ce_loss + l1_reg * l1_reg_loss\n",
        "\n",
        "        # l2_reg_loss = 0\n",
        "        # for param in model.parameters():\n",
        "        #     l2_reg_loss += torch.sum(torch.pow(param, 2))\n",
        "        # loss = ce_loss + l2_reg * l2_reg_loss\n",
        "        \n",
        "        loss = ce_loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "    \n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch_data, batch_labels in val_loader:\n",
        "            outputs = model(batch_data)\n",
        "            loss = criterion(outputs, batch_labels)\n",
        "            val_loss += loss.item()\n",
        "    \n",
        "    # Compute train and val loss\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    val_loss = val_loss / len(val_loader)\n",
        "    \n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    \n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "epochs = range(1, num_epochs + 1)\n",
        "plt.plot(epochs, train_losses, 'b', label='Training Loss')\n",
        "plt.plot(epochs, val_losses, 'r', label='Validation Loss')\n",
        "# plt.title('Training and Validation Loss, l2_reg: ' + str(l2_reg))\n",
        "plt.title('Training and Validation Loss, lr: ' + str(learning_rate))\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# eval on test\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_data, batch_labels in test_loader:\n",
        "        outputs = model(batch_data)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += batch_labels.size(0)\n",
        "        correct += (predicted == batch_labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# subject-independent experiement\n",
        "import numpy as np\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "\n",
        "num_subjects = 15\n",
        "\n",
        "# get all subject ids\n",
        "subject_ids = sorted(list(set(dataset.info[\"subject_id\"])))\n",
        "\n",
        "loo = LeaveOneOut()\n",
        "\n",
        "train_test_splits = []\n",
        "\n",
        "for train_index, test_index in loo.split(subject_ids):\n",
        "    train_subjects = [subject_ids[i] for i in train_index]\n",
        "    test_subject = subject_ids[test_index[0]]\n",
        "    \n",
        "    train_indices = [i for i in range(len(dataset)) if dataset.info[\"subject_id\"][i] in train_subjects]\n",
        "    test_indices = [i for i in range(len(dataset)) if dataset.info[\"subject_id\"][i] == test_subject]\n",
        "    \n",
        "    train_test_splits.append((train_indices, test_indices))\n",
        "\n",
        "for fold, (train_indices, test_indices) in enumerate(train_test_splits):\n",
        "    print(f\"Fold {fold+1}/{len(train_test_splits)}\")\n",
        "    \n",
        "    train_dataset = Subset(dataset, train_indices)\n",
        "    test_dataset = Subset(dataset, test_indices)\n",
        "    \n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    \n",
        "    # model = ..."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
